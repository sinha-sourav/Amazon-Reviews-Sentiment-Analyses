# -*- coding: utf-8 -*-
"""Amazon_Reviews_Sentiment_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F9mT458br6ULml_x2Zhx6dIJZUM7ZgHu
"""

# Importing libraries

!pip install keras

import nltk
from nltk.corpus import twitter_samples
from nltk.corpus import stopwords          # module for stop words that come with NLTK
nltk.download('stopwords')
nltk.download('twitter_samples')
from nltk.stem import PorterStemmer        # module for stemming
from nltk.tokenize import TweetTokenizer   # module for tokenizing strings
nltk.download('punkt') 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import random
import os
from os import getcwd
import seaborn as sns 
import string
import re
import pdb
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import pickle
#import gensim
#from gensim.models import KeyedVectors 
import scipy
import collections
from collections import Counter
#import emoji
import random
import pprint
import IPython 
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import load_model, Model
from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector
from keras.initializers import glorot_uniform
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from keras import backend as K
import json
import csv
import tensorflow_datasets as tfds
print(tf.__version__)
print(np.__version__)
import bz2
import time
import multiprocessing as mp 
mp.cpu_count()
from concurrent import futures
import pickle
import torch
from torch.multiprocessing import Pool

# Defining paths to folders

# sourcepath = "E:\Dropbox\Machine Learning\Side Projects\Amazon Reviews Sentiment Analyses\\amazon_reviews_sentiment_analyses"
# rawdatapath = os.path.join(sourcepath, 'data', 'raw')
# finaldatapath = os.path.join(sourcepath, 'data', 'processed')
# graphpath = os.path.join(sourcepath, 'reports', 'figures')

# Defining filenames
inputtrain = "train.ft.txt.bz2"
inputtest = "test.ft.txt.bz2"
outputtrain = "train"
outputtest = "test"

from google.colab import drive
drive.mount('/content/drive')

# Defining data holder for input 
intrain = []
intest = []

# Loading training and test data 
with bz2.open(os.path.join("drive/MyDrive", inputtrain), 'rt', encoding='utf-8') as f:
    for line in f:
        l = line.strip()
        intrain.append(l)
with bz2.open(os.path.join("drive/MyDrive", inputtest), 'rt', encoding='utf-8') as f:
    for line in f:
        l = line.strip()
        intest.append(l)

# Importing pre-trained GloVe word embeddings 

embeddings_index = {}
with open(os.path.join("drive/MyDrive", 'glove.6B.100d.txt'), encoding='utf-8') as f:
    for line in f:
        values = line.split();
        word = values[0];
        coefs = np.asarray(values[1:], dtype='float32');
        embeddings_index[word] = coefs;

# Checking the number of input training and test examples
numtrain = len(intrain)
numtest = len(intest)

print(f"Number of training examples: {numtrain}")
print(f"Number of test examples: {numtest}")

"""Next, we separate each training and test example, into sentence and labels. Each example has the following string at the very beginning of the example: \_\_label\_\_i, where `i' denotes the number of review stars for that example. After the label, the example contains the title of the review which ends with ':'. So we extract, the title, the label, and the actual review text for each example in both training and test sets. """

train_X = []
train_X_title = []
train_Y = []
test_X = []
test_X_title = []
test_Y = []

for i in np.arange(numtrain):
    train_X.append(re.findall('(?<=: ).*$', intrain[i])[0])
    firststr = re.sub('\:.*','',intrain[i])
    train_X_title.append(re.findall('(?<=__label__[0-9] ).*$', firststr)[0])
    train_Y.append(int(re.findall('[0-9]' ,str(re.findall(r'__label__[0-9]+ ', firststr)[0]))[0]))
for i in np.arange(numtest):
    test_X.append(re.findall('(?<=: ).*$', intest[i])[0])
    firststr = re.sub('\:.*','',intest[i])
    test_X_title.append(re.findall('(?<=__label__[0-9] ).*$', firststr)[0])
    test_Y.append(int(re.findall('[0-9]' ,str(re.findall(r'__label__[0-9]+ ', firststr)[0]))[0]))

# Instead of working with the huge data, we work with 1% of the training set (1% of each of the two labels)
# and similarly we work with 1% of the test set (1% of each of the two labels)

train_ratio = 0.01
test_ratio = 0.01
random.seed(891)

train_X_1 = [review for counter, review in enumerate(train_X) if train_Y[counter] == 1]
train_X_2 = [review for counter, review in enumerate(train_X) if train_Y[counter] == 2]
test_X_1 = [review for counter, review in enumerate(test_X) if test_Y[counter] == 1]
test_X_2 = [review for counter, review in enumerate(test_X) if test_Y[counter] == 2]

subtrain_X_1 = random.choices(train_X_1, k=int(len(train_X_1)*train_ratio))
subtrain_X_2 = random.choices(train_X_2, k=int(len(train_X_2)*train_ratio))
subtest_X_1 = random.choices(test_X_1, k=int(len(test_X_1)*test_ratio))
subtest_X_2 = random.choices(test_X_2, k=int(len(test_X_2)*test_ratio))

subtrain_X = subtrain_X_1+subtrain_X_2
subtest_X = subtest_X_1+subtest_X_2

subtrain_Y = [1 for i in np.arange(len(subtrain_X_1))]+[2 for i in np.arange(len(subtrain_X_2))]
subtest_Y = [1 for i in np.arange(len(subtest_X_1))]+[2 for i in np.arange(len(subtest_X_2))]

# Some Training Examples

for i in np.arange(3):
    print(f"Example {i+1}:\n Review: {subtrain_X[i]}\n Review Label: {subtrain_Y[i]}\n")
    
print(f"Number of training examples and labels extracted: {len(subtrain_X), len(subtrain_Y)}")
print(f"Number of test examples and labels extracted: {len(subtest_X), len(subtest_Y)}")
print(f"Unique labels in training set: {set(subtrain_Y)}")
print(f"Unique labels in test set: {set(subtest_Y)}")

"""Label "1" denotes 1 and 2 stars and label "2" denotes 4 and 5 stars. 3 stars (which might represent neutral reviews) have been removed from the training and test examples.

### Model 1: Naive Bayes Method for Sentiment Classification

First, we preprocess each review by performing the following operations on each tweet:
* Tokenizing (Convert a review to a list of words)
* Lowercasing (Change all characters to lowercase)
* Removing all stopwords and punctuations 
* Stemming (Convert all words into root words)
"""

# Define a function which preprocesses each review
def preprocess_review(review):
    
    # Remove hyperlinks
    review = re.sub(r'https?:\/\/.*[\r\n]*', '', review)
    # Remove hashtags 
    review = re.sub(r'#', '', review)  
    
    # Tokenize review and change to lowercase  
    review_tokens = nltk.word_tokenize(review.lower())
    
    # Now remove stopwords and punctuations
    review_clean = []
    for token in review_tokens:
        if(token not in stopwords.words('spanish') and 
           token not in stopwords.words('english') and 
           token not in string.punctuation):
            review_clean.append(token)
            
    # Now stem words to get only root words
    stemmer = PorterStemmer()
    review_stemmed = []
    for token in review_clean:
        review_stemmed.append(stemmer.stem(token))
        
    return review_stemmed

processed_train = []
processed_test = []

starttime = time.time()

# for i in np.arange(len(subtest_X)):
#     processed_test.append(preprocess_review(subtest_X[i]))
# for i in np.arange(len(subtrain_X)):
#     processed_train.append(preprocess_review(subtrain_X[i]))

pool = Pool()
processed_test = pool.map(preprocess_review, [review for review in subtest_X])
pool.close()

pool = Pool()
processed_train = pool.map(preprocess_review, [review for review in subtrain_X])
pool.close()

# with futures.ProcessPoolExecutor(max_workers=mp.cpu_count()) as pool:
#     res = pool.map(preprocess_review, subtest_X_1, chunksize=mp.cpu_count())

print(time.time()-starttime)

with open(os.path.join("drive/MyDrive",'processed_train.txt'), 'wb') as f:
    pickle.dump(processed_train, f)
with open(os.path.join("drive/MyDrive",'processed_test.txt'), 'wb') as f:
    pickle.dump(processed_test, f)

"""Next, we define a function which creates a dictionary. Each dictionary key is a (word, label) tuple and the key value is the number of times the word appears in the set of reviews with that label. We build this dictionary using only the training set. """

# Define a function to create word frequency 

def gen_wordfreq(xlist, ylist):

  # Initialize an empty dictionary 
  wordfreq = {}
  # Now iterate over each word in each review
  for counter, review in enumerate(xlist):
    for word in review:
      wordfreq[(word, ylist[counter])] = wordfreq.get((word, ylist[counter]), 0) + 1
  # Return the dictionary
  return wordfreq

# Now create the (word, label) frequency dictionary with only the training data. 

print(f"Length of processed training sets of reviews and labels: {len(processed_train), len(subtrain_Y)}")
wordfreq = gen_wordfreq(processed_train, subtrain_Y)
print(f"Size of generated dictionary: {len(wordfreq)}")
print(f"Unique labels in frequency dictionary: {set([label for _,label in wordfreq.keys()])}")

# Now generate a vocabulary and a size of the vocabulary 
vocab = set([key for key,_ in wordfreq.keys()])
V = len(vocab)

# Number of reviews with labels 1 and 2 
D1 = len(subtrain_X_1)
D2 = len(subtrain_X_2)

# Frequency of all words with label 1 
N1 = sum([wordfreq[(word,1)] for word in vocab if (word,1) in wordfreq])
N2 = sum([wordfreq[(word,2)] for word in vocab if (word,2) in wordfreq])

print(f"Length of training vocabulary: {V}")
print(f"Number of reviews with labels 1 and 2: {D1, D2}")
print(f"Total frequency of words in reviews with labels 1 and 2: {N1,N2}")

"""Now 'train' the Naive Bayes Method."""

# Define a function which 'trains' the Naive Bayes Classification Method

def train_NaiveBayes(reviewlist, wordfreq, V, D1, D2, N1, N2):

  y_pred = []
  logprior = np.log(D2)-np.log(D1)

  for review in reviewlist:
    sumloglikelihood = 0.
    for word in review:
      sumloglikelihood += np.log((wordfreq.get((word,2),0)+1)/(N2+V))-np.log((wordfreq.get((word,1),0)+1)/(N1+V))
    y_pred.append(sumloglikelihood+logprior)

  ypred = (np.array(y_pred)>=0).astype(int)*2+(np.array(y_pred)<0).astype(int)*1
  return list(ypred)

# Now find the predicted labels for the training set reviews

ypred_train = train_NaiveBayes(processed_train, wordfreq, V, D1, D2, N1, N2)
print(f"Length of predicted label list for training set reviews: {len(ypred_train)}")

# Now define a function which takes in the actual set of labels, predicted labels, and computes 
# different metrics for classification algorithms

def evaluate_classification(actlab, predlab, predproba, do_ROCAUC):

  accuracy = accuracy_score(actlab, predlab)
  precision = precision_score(actlab, predlab)
  recall = recall_score(actlab, predlab)
  f1 = f1_score(actlab, predlab)
  rocauc = 0.
  if(do_ROCAUC):
    rocauc = roc_auc_score(actlab, predproba) 


  print("Accuracy: {:.3f}".format(accuracy))
  print("Precision: {:.3f}".format(precision))
  print("Recall: {:.3f}".format(recall))
  print("F1 Score: {:.3f}".format(f1))
  if(do_ROCAUC):
    print("ROC AUC Score: {:.3f}".format(rocauc))

  return (accuracy, precision, recall, f1, rocauc)

# Show the performance evaluation of the Naive Bayes method
print("Evaluation of Training Set Classification Using Naive Bayes Method:\n")
NB_train_metrics = evaluate_classification(subtrain_Y, ypred_train, _, False)

# Now apply the Naive Bayes classification method to the test set
ypred_test = train_NaiveBayes(processed_test, wordfreq, V, D1, D2, N1, N2)
print(f"Length of predicted label list for test set reviews: {len(ypred_test)}")

print("Evaluation of Test Set Classification Using Naive Bayes Method:\n")
NB_test_metrics = evaluate_classification(subtest_Y, ypred_test, _, False)

"""## Classification of Reviews Using Sequence Models

In this section of the analyses, we classify reviews by using different types of sequence models. In particular we use the following types of models:

1. Neural Networks
2. Basic Recurrent Neural Networks (RNN)
3. Gated Recurrent Neural Networks (GRU)
4. Long Short Term Memory Unit (LSTM)

For each of these methods, we use word embeddings in the very first layer of the model architecture. We use two types of word embeddings:

* We learn the embeddings as part of the training process. 
* We use pre-trained word embeddings. 
"""

# First we tokenize the training data and use this to create a vocabulary

oovtok = "</oov>"

tokenizer = Tokenizer(oov_token = oovtok)
tokenizer.fit_on_texts(subtrain_X)
word_index = tokenizer.word_index
subtrain_sequences = tokenizer.texts_to_sequences(subtrain_X) 
vocab_size = len(word_index)

print(f"Length of the vocabulary: {vocab_size}")

# Before we pad the sequences, we first need to know the distribution of the length of these sequences

dist_lenseq = []
for seq in subtrain_sequences:
  dist_lenseq.append(len(seq))
perc = list(np.percentile(dist_lenseq, q = [25, 50, 75]))
perc.append(np.mean(dist_lenseq))

fig, ax = plt.subplots(figsize = (8, 6))
g = sns.histplot(dist_lenseq, stat = 'density', element = 'step', binwidth = 15)
ax.set_title('Density Plot of Length of Training Set Reviews', fontsize = 15)
ax.set_xlabel('')
ax.set_ylabel('Density', fontsize = 12)
ax.vlines(perc, 0, 0.012, colors = 'black', linestyles = 'dashed')
ax.annotate("25th percentile: {:0.0f}".format(perc[0]), (perc[0]-41,0.012), size = 8)
ax.annotate("Median: {:0.0f}".format(perc[1]), (perc[1]-25,0.0115), size = 8)
ax.annotate("Mean: {:0.0f}".format(perc[3]), (perc[3]+2,0.0115), size = 8)
ax.annotate("75th percentile: {:0.0f}".format(perc[2]), (perc[2]+2,0.012), size = 8);

"""Since the 75th percentile of the length of the tokenized training sequences is 105, I will truncate all tokenized sequences, from the end, to a length of 128. For tokenized sequences less than 128 tokens in length, I will pad the sequences at the end. This is applied to both the training and test sets of reviews. """

max_length = 128
trunct_type = 'post'
pad_type = 'post'

subtrain_padded = pad_sequences(subtrain_sequences, 
                                padding = pad_type, truncating = trunct_type, maxlen = max_length)

print(f"Checking to see that all padded training reviews have the same length as max_length above:")
assert np.max([len(padded) for padded in subtrain_padded])==max_length
print("Verified")

# Now convert the test set into sequences and then pad them as well

subtest_sequences = tokenizer.texts_to_sequences(subtest_X)
subtest_padded = pad_sequences(subtest_sequences, 
                               padding = pad_type, truncating = trunct_type, maxlen = max_length)

print(f"Checking to see that all padded test reviews have the same length as max_length above:")
assert np.max([len(padded) for padded in subtest_padded])==max_length
print("Verified")

subtrain_Y = [i-1 for i in subtrain_Y]
subtest_Y = [i-1 for i in subtest_Y]

# Define an embedding dimension

embedding_dim = 100

# Next, we define a callback function which will terminate the training when a certain value of accuracy is reached

accuracy_threshold = 0.96
global final_epoch 
final_epoch = 0
class myCallBack(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs = {}):
        if(logs.get('accuracy') > accuracy_threshold):
            print('Desired Accuracy Threshold Reached. Aborting Training. Epoch reached: {}'.format(epoch))
            global final_epoch
            final_epoch = epoch
            self.model.stop_training = True
callbacks = myCallBack()

"""### 3.a.1. Neural Network With Trainable Word Embeddings """

num_units = 32

NN_model_a = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(num_units, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
NN_model_a.summary()
NN_model_a.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

tf.random.set_seed(193)
num_epochs = 30
history_NN_model_a = NN_model_a.fit(subtrain_padded, np.array(subtrain_Y), 
                         validation_data = (subtest_padded, np.array(subtest_Y)), 
                         epochs = num_epochs, verbose = 2, callbacks = [callbacks])

# Define a function which plots the training and test loss and accuracy 

def plot_training(history, titname):

  fig, (ax1, ax2) = plt.subplots(1,2, figsize = (17, 6))
  ax1.plot(history.history['accuracy'], color='navy')
  ax1.plot(history.history['val_accuracy'], color='red')
  ax1.set_xlabel("Epochs")
  ax1.set_ylabel('Accuracy')
  ax1.legend(['Accuracy', 'val_Accuracy'])

  ax2.plot(history.history['loss'], color='navy')
  ax2.plot(history.history['val_loss'], color='red')
  ax2.set_xlabel("Epochs")
  ax2.set_ylabel('Loss')
  ax2.legend(['Loss', 'val_Loss'])

  plt.tight_layout()    
  plt.suptitle(titname, fontsize = 15, y = 1.1)
  plt.show()

if(final_epoch>3):
  plot_training(history_NN_model_a, 'Basic Neural Network With Trainable Word Embeddings')

## Now we predict labels and the probability of those labels in the test set
Ypredproba_NN_model_a = list(NN_model_a.predict(subtest_padded).squeeze())
Ypred_NN_model_a = np.array([i>=0.5 for i in Ypredproba_NN_model_a], dtype = 'int')

print(f"Evaluation of Test Set Classification Using Neural Networks With Trainable Word Embeddings:")
NN_model_a_test_metrics = evaluate_classification(subtest_Y, Ypred_NN_model_a, Ypredproba_NN_model_a, True)

"""### 3.a.2. Neural Network With Pre-Trained Word Embeddings"""

pretrain_embedding_dim = embeddings_index[list(embeddings_index.keys())[0]].shape[0]
embeddings_matrix = np.zeros((vocab_size+1,pretrain_embedding_dim)) 

for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None: 
        embeddings_matrix[i] = embedding_vector

num_units = 32

NN_model_b = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(num_units, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
NN_model_b.summary()
NN_model_b.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

tf.random.set_seed(193)
final_epoch = 0
num_epochs = 30
history_NN_model_b = NN_model_b.fit(subtrain_padded, np.array(subtrain_Y), 
                         validation_data = (subtest_padded, np.array(subtest_Y)), 
                         epochs = num_epochs, verbose = 2, callbacks = [callbacks])

if(final_epoch>3):
  plot_training(history_NN_model_b, 'Basic Neural Network With Pre-Trained Word Embeddings')

## Now we predict labels and the probability of those labels in the test set
Ypredproba_NN_model_b = list(NN_model_b.predict(subtest_padded).squeeze())
Ypred_NN_model_b = np.array([i>=0.5 for i in Ypredproba_NN_model_b], dtype = 'int')

print(f"Evaluation of Test Set Classification Using Neural Networks With Pre-Trained Word Embeddings:")
NN_model_b_test_metrics = evaluate_classification(subtest_Y, Ypred_NN_model_b, Ypredproba_NN_model_b, True)

"""### 3.b.1. Single Layer Bidirectional RNN With GRU Units And Trainable Word Embeddings"""

num_units = 32
num_RNN_units = 64

GRU_model_a = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(num_RNN_units)),
    tf.keras.layers.Dense(num_units, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
GRU_model_a.summary()
GRU_model_a.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

tf.random.set_seed(193)
final_epoch = 0
num_epochs = 30
history_GRU_model_a = GRU_model_a.fit(subtrain_padded, np.array(subtrain_Y), 
                         validation_data = (subtest_padded, np.array(subtest_Y)), 
                         epochs = num_epochs, verbose = 2, callbacks = [callbacks])

if(final_epoch>3):
  plot_training(history_GRU_model_a, 'Bidirectional Gated Recurrent Neural Network With Trainable Word Embeddings')

## Now we predict labels and the probability of those labels in the test set
Ypredproba_GRU_model_a = list(GRU_model_a.predict(subtest_padded).squeeze())
Ypred_GRU_model_a = np.array([i>=0.5 for i in Ypredproba_GRU_model_a], dtype = 'int')

print(f"Evaluation of Test Set Classification Using Gated Recurrent Neural Networks With Trainable Word Embeddings:")
GRU_model_a_test_metrics = evaluate_classification(subtest_Y, Ypred_GRU_model_a, Ypredproba_GRU_model_a, True)

"""### 3.b.2. Single Layer Bidirectional RNN With GRU Units And Pre-Trained Word Embeddings 

"""

num_units = 32
num_RNN_units = 64

GRU_model_b = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(num_RNN_units)),
    tf.keras.layers.Dense(num_units, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
GRU_model_b.summary()
GRU_model_b.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

tf.random.set_seed(193)
final_epoch = 0
num_epochs = 30
history_GRU_model_b = GRU_model_b.fit(subtrain_padded, np.array(subtrain_Y), 
                         validation_data = (subtest_padded, np.array(subtest_Y)), 
                         epochs = num_epochs, verbose = 2, callbacks = [callbacks])

if(final_epoch>3):
  plot_training(history_GRU_model_b, 'Bidirectional Gated Recurrent Neural Network With Pre-Trained Word Embeddings')

## Now we predict labels and the probability of those labels in the test set
Ypredproba_GRU_model_b = list(GRU_model_b.predict(subtest_padded).squeeze())
Ypred_GRU_model_b = np.array([i>=0.5 for i in Ypredproba_GRU_model_b], dtype = 'int')

print(f"Evaluation of Test Set Classification Using Gated Recurrent Neural Networks With Pre-Trained Word Embeddings:")
GRU_model_b_test_metrics = evaluate_classification(subtest_Y, Ypred_GRU_model_b, Ypredproba_GRU_model_b, True)

"""### 3.c.1. Single Layer RNN With LSTM Units And Trainable Word Embeddings"""

num_units = 32
num_RNN_units = 64

LSTM_model_a = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(num_RNN_units)),
    tf.keras.layers.Dense(num_units, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
LSTM_model_a.summary()
LSTM_model_a.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

tf.random.set_seed(193)
final_epoch = 0
num_epochs = 30
history_LSTM_model_a = LSTM_model_a.fit(subtrain_padded, np.array(subtrain_Y), 
                         validation_data = (subtest_padded, np.array(subtest_Y)), 
                         epochs = num_epochs, verbose = 2, callbacks = [callbacks])

if(final_epoch>3):
  plot_training(history_LSTM_model_a, 'Bidirectional LSTM Recurrent Neural Network With Trainable Word Embeddings')

## Now we predict labels and the probability of those labels in the test set
Ypredproba_LSTM_model_a = list(LSTM_model_a.predict(subtest_padded).squeeze())
Ypred_LSTM_model_a = np.array([i>=0.5 for i in Ypredproba_LSTM_model_a], dtype = 'int')

print(f"Evaluation of Test Set Classification Using LSTM Recurrent Neural Networks With Trainable Word Embeddings:")
LSTM_model_a_test_metrics = evaluate_classification(subtest_Y, Ypred_LSTM_model_a, Ypredproba_LSTM_model_a, True)

"""### 3.c.2. Single Layer Bidirectional RNN With LSTM Units And Pre-Trained Word Embeddings"""

num_units = 32
num_RNN_units = 64

LSTM_model_b = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(num_RNN_units)),
    tf.keras.layers.Dense(num_units, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
LSTM_model_b.summary()
LSTM_model_b.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

tf.random.set_seed(193)
final_epoch = 0
num_epochs = 30
history_LSTM_model_b = LSTM_model_b.fit(subtrain_padded, np.array(subtrain_Y), 
                         validation_data = (subtest_padded, np.array(subtest_Y)), 
                         epochs = num_epochs, verbose = 2, callbacks = [callbacks])

if(final_epoch>3):
  plot_training(history_LSTM_model_b, 'Bidirectional LSTM Recurrent Neural Network With Pre-Trained Word Embeddings')

## Now we predict labels and the probability of those labels in the test set
Ypredproba_LSTM_model_b = list(LSTM_model_b.predict(subtest_padded).squeeze())
Ypred_LSTM_model_b = np.array([i>=0.5 for i in Ypredproba_LSTM_model_b], dtype = 'int')

print(f"Evaluation of Test Set Classification Using LSTM Recurrent Neural Networks With Pre-Trained Word Embeddings:")
LSTM_model_b_test_metrics = evaluate_classification(subtest_Y, Ypred_LSTM_model_b, Ypredproba_LSTM_model_b, True)

# Now generate lists of different metrics and models 

listmodel = ['Naive Bayes',
             'Neural Network - Trainable Embeddings','Neural Network - Pre-Trained Embeddings',
             'GRU - Trainable Embeddings','GRU - Pre-Trained Embeddings',
             'LSTM - Trainable Embeddings','LSTM - Pre-Trained Embeddings']
listaccuracy = [NB_test_metrics[0],
                NN_model_a_test_metrics[0], NN_model_b_test_metrics[0],
                GRU_model_a_test_metrics[0], GRU_model_b_test_metrics[0], 
                LSTM_model_a_test_metrics[0], LSTM_model_b_test_metrics[0]]
listprecision = [NB_test_metrics[1],
                NN_model_a_test_metrics[1], NN_model_b_test_metrics[1],
                GRU_model_a_test_metrics[1], GRU_model_b_test_metrics[1], 
                LSTM_model_a_test_metrics[1], LSTM_model_b_test_metrics[1]]
listrecall = [NB_test_metrics[2],
                NN_model_a_test_metrics[2], NN_model_b_test_metrics[2],
                GRU_model_a_test_metrics[2], GRU_model_b_test_metrics[2], 
                LSTM_model_a_test_metrics[2], LSTM_model_b_test_metrics[2]]
listf1 = [NB_test_metrics[3],
                NN_model_a_test_metrics[3], NN_model_b_test_metrics[3],
                GRU_model_a_test_metrics[3], GRU_model_b_test_metrics[3], 
                LSTM_model_a_test_metrics[3], LSTM_model_b_test_metrics[3]]
listrocauc = [np.nan,
                NN_model_a_test_metrics[4], NN_model_b_test_metrics[4],
                GRU_model_a_test_metrics[4], GRU_model_b_test_metrics[4], 
                LSTM_model_a_test_metrics[4], LSTM_model_b_test_metrics[4]]

# Now plot the evaluation metrics

fig, ax = plt.subplots(figsize = (12, 9))
ax.scatter(listmodel, listaccuracy, marker = 'o', s = 40, color = 'black', label = '')
ax.plot(listmodel, listaccuracy, color = 'grey', label = 'Accuracy')
ax.scatter(listmodel, listprecision, marker = 'o', s = 40, color = 'red', label = '')
ax.plot(listmodel, listprecision, color = 'salmon', label = 'Precision')
ax.scatter(listmodel, listrecall, marker = 'o', s = 40, color = 'darkgreen', label = '')
ax.plot(listmodel, listrecall, color = 'limegreen', label = 'Recall')
ax.scatter(listmodel, listf1, marker = 'o', s = 40, color = 'darkblue', label = '')
ax.plot(listmodel, listf1, color = 'cornflowerblue', label = 'F1 Score')
ax.scatter(listmodel, listrocauc, marker = 'o', s = 40, color = 'darkmagenta', label = '')
ax.plot(listmodel, listrocauc, color = 'mediumorchid', label = 'ROC-AUC Score')
ax.set_ylim([0.65, 1.0])
ax.set_xticklabels(listmodel, rotation = 270, fontsize = 12)
ax.tick_params(labelsize = 12)
plt.grid(which = 'major', color = 'grey', linestyle = 'dashed', linewidth = 0.5)
plt.legend(loc = 'lower left', frameon = True, fancybox = True, fontsize = 12)
ax.set_title('Evaluation Of Sentiment Classification Models', fontsize = 15);
plt.savefig(os.path.join("drive/MyDrive", 'PredictionMetricsComparison_ClassifierModels.png'), 
           bbox_inches = 'tight')

